# ğŸ¦™ Ollama Module for Roblox

A super chill way to talk to your local Ollama AI using Lua in Roblox! Just make sure HTTP requests are enabled, and youâ€™re ready to vibe with your LLM.

> ğŸ”— Default API: `http://localhost:11434/api/chat`  
> ğŸ§  Default Model: `"llama2"`  
> ğŸš« Note: Streaming is **not implemented** yet!

---

## ğŸ“¦ Setup

```lua
local ReplicatedStorage = game:GetService("ReplicatedStorage")
local Ollama = require(ReplicatedStorage:WaitForChild("Ollama"))

local session = Ollama:Create({
	Model = "llama3", -- Optional override
	API_URL = "http://localhost:11434/api/chat", -- Optional override
})
```

---

## âœ¨ Functions (explained simply)

### ğŸ›  `:Create(config)`
Creates a new Ollama session. You can optionally override the default config.

### ğŸ§  `:prompt(userMessage)`
Sends a message to the AI and returns its response. Also adds both your message and the botâ€™s reply to history.

### ğŸ” `:getReplyOnly(userMessage)`
Sends a message and gets a response, but **does not** save anything to history.

### ğŸ§½ `:reset()` / `:clearHistory()`
Wipes all stored messages from chat history. (Basically the same.)

### ğŸ§¼ `:clearMessagesByRole(role)`
Removes all messages by a specific role (like `"user"`, `"assistant"`, or `"system"`).

### âœï¸ `:addMessage(role, content)`
Adds a message manually to the history. Great for advanced control.

### ğŸ“¢ `:inform(systemContent)`
Adds or updates the system message. (e.g., â€œYou are a helpful assistant.â€)

### ğŸ§¹ `:undoLast()`
Deletes the last message from the history (like Ctrl+Z).

### ğŸ§³ `:importHistory(data)`
Loads history from a table or a JSON string.

### ğŸ“¤ `:exportHistoryJSON()`
Exports current history as a JSON string. Great for saving.

### ğŸ“¡ `:ping()`
Checks if your Ollama server is alive. Returns `true` if reachable.

### ğŸ’¬ `:getLatestReply()`
Grabs the latest assistant message from history (if it exists).

### ğŸ”§ `:setConfig(key, value)`
Updates one of the config values like `"Model"` or `"Stream"`.

### ğŸ§  `:getReplyWithCustomContext(messages)`
Sends a custom list of messages to the model and gets a reply (without affecting history).

### ğŸ§™ `:getReplyFromSystemPrompt(prompt)`
Sends a special prompt with both system + user content:
```lua
{
	System = "You are a robot",
	User = "Whatâ€™s 2 + 2?"
}
```

### ğŸ“š `:getHistory()`
Returns the current history table.

---

## ğŸ§ª Example

```lua
session:inform("You are a Roblox Lua genius.")
local reply = session:prompt("What's a metatable?")
print(reply)
```

---

## ğŸš§ Notes

- Make sure **HTTP requests are enabled** in your game settings.
- `Stream = true` does nothing yet â€“ it's just there for future updates.
- Default model is `"llama2"`, but you can change it to any supported Ollama model.

---

Made with ğŸ’™ by [Crey].
